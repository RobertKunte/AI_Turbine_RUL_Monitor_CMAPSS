experiment_id: fd004_transformer_encoder_v1
fd_id: FD004

# Pure Transformer-Encoder EOL+HI model on FD004.
# Uses the Phase-4 residual feature pipeline (~464 features) and the
# same multi-task EOL+HI loss as the LSTM + Attention baselines, but
# replaces the recurrent encoder with a Transformer encoder + attention pooling.

model_class: EOLFullTransformerEncoder
encoder_type: transformer_encoder_v1

model:
  # Sequence settings
  past_len: 30
  max_rul: 125

  # Transformer encoder
  d_model: 64
  n_heads: 4
  num_layers: 3
  dim_feedforward: 256
  dropout: 0.1
  max_seq_len: 300

  # Condition handling (FD004 multi-condition)
  use_condition_embedding: true
  num_conditions: 7
  cond_emb_dim: 4

data:
  use_physical_features: true
  use_multi_scale_features: true
  use_residual_features: true
  condition_wise_scaling: true
  condition_embeddings: true

  unit_col: UnitNumber
  cycle_col: TimeInCycles
  rul_col: RUL

loss:
  # RUL loss weighting (as in Phase-4)
  rul_beta: 45.0

  # Health Index loss weights
  health_loss_weight: 0.30
  mono_late_weight: 0.02
  mono_global_weight: 0.003
  hi_condition_calib_weight: 0.0

  hi_plateau_threshold: 80.0
  hi_eol_threshold: 25.0
  hi_eol_weight: 4.0

training:
  batch_size: 256
  lr: 0.0001
  num_epochs: 80
  weight_decay: 0.0001
  optimizer: adam
  scheduler: null
  patience: 10
  seed: 42



