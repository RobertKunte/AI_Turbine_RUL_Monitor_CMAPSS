{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import torch  # type: ignore[import]\n",
        "    import torch.nn as nn  # type: ignore[import]\n",
        "    from torch.utils.data import TensorDataset, DataLoader, random_split  # type: ignore[import]\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\n",
        "        \"PyTorch is required for this notebook. Please install torch.\"\n",
        "    ) from exc\n",
        "\n",
        "try:\n",
        "    from sklearn.preprocessing import MinMaxScaler, StandardScaler  # type: ignore[import]\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\n",
        "        \"scikit-learn is required for this notebook. Please install scikit-learn.\"\n",
        "    ) from exc\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "import sys\n",
        "root_dir = os.path.abspath(\"..\")\n",
        "if root_dir not in sys.path:\n",
        "    sys.path.append(root_dir)\n",
        "\n",
        "from src.config import (\n",
        "    CMAPSS_DATASETS,\n",
        "    MAX_RUL,\n",
        "    SEQUENCE_LENGTH,\n",
        "    HIDDEN_SIZE,\n",
        "    NUM_LAYERS,\n",
        "    OUTPUT_SIZE,\n",
        "    LEARNING_RATE,\n",
        "    NUM_EPOCHS,\n",
        "    GLOBAL_FEATURE_COLS,\n",
        "    GLOBAL_DROP_COLS \n",
        ")\n",
        "\n",
        "from src.data_loading import load_cmapps_subset\n",
        "from src.additional_features import create_physical_features\n",
        "from src.eol_full_lstm import (\n",
        "    build_full_eol_sequences_from_df,\n",
        "    create_full_dataloaders,\n",
        "    EOLFullLSTM,\n",
        "    train_eol_full_lstm,\n",
        "    evaluate_eol_full_lstm,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full-Trajectory LSTM für EOL-Prediction (FD001-FD004 - Separate Training)\n",
        "\n",
        "Dieses Notebook trainiert für jedes CMAPSS-Dataset (FD001, FD002, FD003, FD004) **separat** ein LSTM-Modell für EOL-Prediction.\n",
        "\n",
        "**Features:**\n",
        "- Sliding-Window über alle Zyklen jeder Engine\n",
        "- RUL wird auf max_rul=125 gecappt (NASA-Style)\n",
        "- Engine-basierter Split (keine Data Leakage)\n",
        "- NASA PHM08 Score Evaluation\n",
        "- **Vergleichstabelle** aller Metriken am Ende\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Konfiguration: Datasets und Hyperparameter\n",
        "# ===================================================================\n",
        "FD_DATASETS = [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
        "\n",
        "# Training-Parameter (gleich für alle Datasets)\n",
        "TRAINING_CONFIG = {\n",
        "    \"past_len\": 30,\n",
        "    \"max_rul\": 125,\n",
        "    \"batch_size\": 256,\n",
        "    \"engine_train_ratio\": 0.8,\n",
        "    \"num_epochs\": 80,\n",
        "    \"lr\": 1e-4,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"patience\": 8,\n",
        "    \"hidden_dim\": 64,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"bidirectional\": False,\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training Configuration\")\n",
        "print(\"=\" * 60)\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Dictionary zum Speichern der Ergebnisse für jeden Dataset\n",
        "# ===================================================================\n",
        "results_dict = {}\n",
        "\n",
        "for fd_id in FD_DATASETS:\n",
        "    results_dict[fd_id] = {\n",
        "        \"model\": None,\n",
        "        \"history\": None,\n",
        "        \"metrics\": None,\n",
        "        \"scaler\": None,\n",
        "        \"num_engines\": None,\n",
        "        \"num_samples\": None,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Training-Loop: Jedes Dataset separat trainieren\n",
        "# ===================================================================\n",
        "\n",
        "for fd_id in FD_DATASETS:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Processing {fd_id}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # -------------------------------------------------------------------\n",
        "    # 1. Daten laden\n",
        "    # -------------------------------------------------------------------\n",
        "    print(f\"\\n[{fd_id}] Step 1: Loading data...\")\n",
        "    df_train, _, _ = load_cmapps_subset(\n",
        "        fd_id,\n",
        "        max_rul=None,  # Kein Clipping im Train (wird später in build_full_eol_sequences gecappt)\n",
        "        clip_train=False,\n",
        "        clip_test=True,  # Test weiter clampen für NASA-Score\n",
        "    )\n",
        "    df_train = create_physical_features(df_train)\n",
        "    \n",
        "    # FD_ID als numerisches Feature (für spätere Verwendung, falls nötig)\n",
        "    fd_id_map = {\"FD001\": 0, \"FD002\": 1, \"FD003\": 2, \"FD004\": 3}\n",
        "    df_train[\"FD_ID\"] = fd_id_map[fd_id]\n",
        "    \n",
        "    print(f\"  Loaded: {len(df_train)} rows, {df_train['UnitNumber'].nunique()} engines\")\n",
        "    results_dict[fd_id][\"num_engines\"] = df_train['UnitNumber'].nunique()\n",
        "    \n",
        "    # -------------------------------------------------------------------\n",
        "    # 2. Feature-Liste definieren\n",
        "    # -------------------------------------------------------------------\n",
        "    print(f\"\\n[{fd_id}] Step 2: Defining features...\")\n",
        "    numeric_cols = df_train[GLOBAL_FEATURE_COLS].select_dtypes(\n",
        "        include=[\"number\"]\n",
        "    ).columns.tolist()\n",
        "    feature_cols = numeric_cols\n",
        "    print(f\"  Using {len(feature_cols)} features\")\n",
        "    \n",
        "    # -------------------------------------------------------------------\n",
        "    # 3. Full-Trajectory Sequenzen bauen\n",
        "    # -------------------------------------------------------------------\n",
        "    print(f\"\\n[{fd_id}] Step 3: Building sequences...\")\n",
        "    X_full, y_full, unit_ids_full = build_full_eol_sequences_from_df(\n",
        "        df=df_train,\n",
        "        feature_cols=feature_cols,\n",
        "        past_len=TRAINING_CONFIG[\"past_len\"],\n",
        "        max_rul=TRAINING_CONFIG[\"max_rul\"],\n",
        "        unit_col=\"UnitNumber\",\n",
        "        cycle_col=\"TimeInCycles\",\n",
        "        rul_col=\"RUL\",\n",
        "    )\n",
        "    print(f\"  Built: {X_full.shape[0]} sequences\")\n",
        "    results_dict[fd_id][\"num_samples\"] = X_full.shape[0]\n",
        "    \n",
        "    # -------------------------------------------------------------------\n",
        "    # 4. Dataloaders erstellen (Engine-basierter Split)\n",
        "    # -------------------------------------------------------------------\n",
        "    print(f\"\\n[{fd_id}] Step 4: Creating dataloaders...\")\n",
        "    train_loader, val_loader, scaler, train_unit_ids, val_unit_ids = create_full_dataloaders(\n",
        "        X=X_full,\n",
        "        y=y_full,\n",
        "        unit_ids=unit_ids_full,\n",
        "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "        engine_train_ratio=TRAINING_CONFIG[\"engine_train_ratio\"],\n",
        "        shuffle_engines=True,\n",
        "        random_seed=42,\n",
        "    )\n",
        "    results_dict[fd_id][\"scaler\"] = scaler\n",
        "    \n",
        "    # -------------------------------------------------------------------\n",
        "    # 5. Modell initialisieren\n",
        "    # -------------------------------------------------------------------\n",
        "    print(f\"\\n[{fd_id}] Step 5: Initializing model...\")\n",
        "    model = EOLFullLSTM(\n",
        "        input_dim=len(feature_cols),\n",
        "        hidden_dim=TRAINING_CONFIG[\"hidden_dim\"],\n",
        "        num_layers=TRAINING_CONFIG[\"num_layers\"],\n",
        "        dropout=TRAINING_CONFIG[\"dropout\"],\n",
        "        bidirectional=TRAINING_CONFIG[\"bidirectional\"],\n",
        "    )\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  Model parameters: {num_params:,}\")\n",
        "    \n",
        "    # -------------------------------------------------------------------\n",
        "    # 6. Training\n",
        "    # -------------------------------------------------------------------\n",
        "    print(f\"\\n[{fd_id}] Step 6: Training...\")\n",
        "    model, history = train_eol_full_lstm(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "        lr=TRAINING_CONFIG[\"lr\"],\n",
        "        weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
        "        patience=TRAINING_CONFIG[\"patience\"],\n",
        "        device=device,\n",
        "        results_dir=f\"../results/eol_full_lstm_single/{fd_id.lower()}\",\n",
        "        run_name=fd_id.lower(),\n",
        "    )\n",
        "    results_dict[fd_id][\"model\"] = model\n",
        "    results_dict[fd_id][\"history\"] = history\n",
        "    \n",
        "    # -------------------------------------------------------------------\n",
        "    # 7. Evaluation\n",
        "    # -------------------------------------------------------------------\n",
        "    print(f\"\\n[{fd_id}] Step 7: Evaluating...\")\n",
        "    metrics = evaluate_eol_full_lstm(\n",
        "        model=model,\n",
        "        val_loader=val_loader,\n",
        "        device=device,\n",
        "    )\n",
        "    results_dict[fd_id][\"metrics\"] = metrics\n",
        "    \n",
        "    print(f\"\\n[{fd_id}] Completed!\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Vergleichstabelle: Pointwise Metriken\n",
        "# ===================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON TABLE: Pointwise Metrics (all validation samples)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "pointwise_data = []\n",
        "for fd_id in FD_DATASETS:\n",
        "    m = results_dict[fd_id][\"metrics\"][\"pointwise\"]\n",
        "    pointwise_data.append({\n",
        "        \"Dataset\": fd_id,\n",
        "        \"RMSE\": f\"{m['rmse']:.4f}\",\n",
        "        \"MAE\": f\"{m['mae']:.4f}\",\n",
        "        \"Bias\": f\"{m['bias']:.4f}\",\n",
        "        \"R²\": f\"{m['r2']:.4f}\",\n",
        "        \"NASA Score (sum)\": f\"{results_dict[fd_id]['metrics']['nasa_pointwise']['score_sum']:.2f}\",\n",
        "        \"NASA Score (mean)\": f\"{results_dict[fd_id]['metrics']['nasa_pointwise']['score_mean']:.4f}\",\n",
        "    })\n",
        "\n",
        "df_pointwise = pd.DataFrame(pointwise_data)\n",
        "print(\"\\n\" + df_pointwise.to_string(index=False))\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Vergleichstabelle: EOL Metriken (per engine, last cycle)\n",
        "# ===================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON TABLE: EOL Metrics (per engine, last cycle)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "eol_data = []\n",
        "for fd_id in FD_DATASETS:\n",
        "    if \"eol\" in results_dict[fd_id][\"metrics\"]:\n",
        "        m = results_dict[fd_id][\"metrics\"][\"eol\"]\n",
        "        eol_data.append({\n",
        "            \"Dataset\": fd_id,\n",
        "            \"Num Engines\": results_dict[fd_id][\"num_engines\"],\n",
        "            \"RMSE_eol\": f\"{m['rmse']:.4f}\",\n",
        "            \"MAE_eol\": f\"{m['mae']:.4f}\",\n",
        "            \"Bias_eol\": f\"{m['bias']:.4f}\",\n",
        "            \"NASA Score (sum)\": f\"{m['nasa_score_sum']:.2f}\",\n",
        "            \"NASA Score (mean)\": f\"{m['nasa_score_mean']:.4f}\",\n",
        "        })\n",
        "    else:\n",
        "        eol_data.append({\n",
        "            \"Dataset\": fd_id,\n",
        "            \"Num Engines\": results_dict[fd_id][\"num_engines\"],\n",
        "            \"RMSE_eol\": \"N/A\",\n",
        "            \"MAE_eol\": \"N/A\",\n",
        "            \"Bias_eol\": \"N/A\",\n",
        "            \"NASA Score (sum)\": \"N/A\",\n",
        "            \"NASA Score (mean)\": \"N/A\",\n",
        "        })\n",
        "\n",
        "df_eol = pd.DataFrame(eol_data)\n",
        "print(\"\\n\" + df_eol.to_string(index=False))\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Vergleichstabelle: Dataset-Statistiken\n",
        "# ===================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON TABLE: Dataset Statistics\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "stats_data = []\n",
        "for fd_id in FD_DATASETS:\n",
        "    stats_data.append({\n",
        "        \"Dataset\": fd_id,\n",
        "        \"Num Engines\": results_dict[fd_id][\"num_engines\"],\n",
        "        \"Num Samples\": results_dict[fd_id][\"num_samples\"],\n",
        "        \"Samples/Engine\": f\"{results_dict[fd_id]['num_samples'] / results_dict[fd_id]['num_engines']:.1f}\",\n",
        "    })\n",
        "\n",
        "df_stats = pd.DataFrame(stats_data)\n",
        "print(\"\\n\" + df_stats.to_string(index=False))\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Kombinierte Vergleichstabelle (alle Metriken)\n",
        "# ===================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPREHENSIVE COMPARISON TABLE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "comprehensive_data = []\n",
        "for fd_id in FD_DATASETS:\n",
        "    m_point = results_dict[fd_id][\"metrics\"][\"pointwise\"]\n",
        "    m_nasa_point = results_dict[fd_id][\"metrics\"][\"nasa_pointwise\"]\n",
        "    \n",
        "    row = {\n",
        "        \"Dataset\": fd_id,\n",
        "        \"Engines\": results_dict[fd_id][\"num_engines\"],\n",
        "        \"Samples\": results_dict[fd_id][\"num_samples\"],\n",
        "        # Pointwise\n",
        "        \"RMSE (point)\": f\"{m_point['rmse']:.4f}\",\n",
        "        \"MAE (point)\": f\"{m_point['mae']:.4f}\",\n",
        "        \"R² (point)\": f\"{m_point['r2']:.4f}\",\n",
        "        \"NASA (point, mean)\": f\"{m_nasa_point['score_mean']:.4f}\",\n",
        "    }\n",
        "    \n",
        "    # EOL Metriken (falls verfügbar)\n",
        "    if \"eol\" in results_dict[fd_id][\"metrics\"]:\n",
        "        m_eol = results_dict[fd_id][\"metrics\"][\"eol\"]\n",
        "        row.update({\n",
        "            \"RMSE (EOL)\": f\"{m_eol['rmse']:.4f}\",\n",
        "            \"MAE (EOL)\": f\"{m_eol['mae']:.4f}\",\n",
        "            \"NASA (EOL, mean)\": f\"{m_eol['nasa_score_mean']:.4f}\",\n",
        "        })\n",
        "    else:\n",
        "        row.update({\n",
        "            \"RMSE (EOL)\": \"N/A\",\n",
        "            \"MAE (EOL)\": \"N/A\",\n",
        "            \"NASA (EOL, mean)\": \"N/A\",\n",
        "        })\n",
        "    \n",
        "    comprehensive_data.append(row)\n",
        "\n",
        "df_comprehensive = pd.DataFrame(comprehensive_data)\n",
        "print(\"\\n\" + df_comprehensive.to_string(index=False))\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Visualisierung: RMSE Vergleich\n",
        "# ===================================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Pointwise RMSE\n",
        "ax1 = axes[0]\n",
        "pointwise_rmse = [results_dict[fd][\"metrics\"][\"pointwise\"][\"rmse\"] for fd in FD_DATASETS]\n",
        "bars1 = ax1.bar(FD_DATASETS, pointwise_rmse, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "ax1.set_ylabel(\"RMSE (cycles)\")\n",
        "ax1.set_title(\"Pointwise RMSE (all validation samples)\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "for i, (fd, val) in enumerate(zip(FD_DATASETS, pointwise_rmse)):\n",
        "    ax1.text(i, val, f\"{val:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "# EOL RMSE (falls verfügbar)\n",
        "ax2 = axes[1]\n",
        "eol_rmse = []\n",
        "eol_labels = []\n",
        "for fd in FD_DATASETS:\n",
        "    if \"eol\" in results_dict[fd][\"metrics\"]:\n",
        "        eol_rmse.append(results_dict[fd][\"metrics\"][\"eol\"][\"rmse\"])\n",
        "        eol_labels.append(fd)\n",
        "    else:\n",
        "        eol_rmse.append(0)\n",
        "        eol_labels.append(fd)\n",
        "\n",
        "if any(eol_rmse):\n",
        "    bars2 = ax2.bar(eol_labels, eol_rmse, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "    ax2.set_ylabel(\"RMSE (cycles)\")\n",
        "    ax2.set_title(\"EOL RMSE (per engine, last cycle)\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    for i, (fd, val) in enumerate(zip(eol_labels, eol_rmse)):\n",
        "        if val > 0:\n",
        "            ax2.text(i, val, f\"{val:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Visualisierung: NASA Score Vergleich\n",
        "# ===================================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Pointwise NASA Score (mean)\n",
        "ax1 = axes[0]\n",
        "nasa_point_mean = [results_dict[fd][\"metrics\"][\"nasa_pointwise\"][\"score_mean\"] for fd in FD_DATASETS]\n",
        "bars1 = ax1.bar(FD_DATASETS, nasa_point_mean, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "ax1.set_ylabel(\"NASA Score (mean)\")\n",
        "ax1.set_title(\"Pointwise NASA Score (mean, all validation samples)\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "for i, (fd, val) in enumerate(zip(FD_DATASETS, nasa_point_mean)):\n",
        "    ax1.text(i, val, f\"{val:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "# EOL NASA Score (mean, falls verfügbar)\n",
        "ax2 = axes[1]\n",
        "eol_nasa_mean = []\n",
        "eol_labels = []\n",
        "for fd in FD_DATASETS:\n",
        "    if \"eol\" in results_dict[fd][\"metrics\"]:\n",
        "        eol_nasa_mean.append(results_dict[fd][\"metrics\"][\"eol\"][\"nasa_score_mean\"])\n",
        "        eol_labels.append(fd)\n",
        "    else:\n",
        "        eol_nasa_mean.append(0)\n",
        "        eol_labels.append(fd)\n",
        "\n",
        "if any(eol_nasa_mean):\n",
        "    bars2 = ax2.bar(eol_labels, eol_nasa_mean, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "    ax2.set_ylabel(\"NASA Score (mean)\")\n",
        "    ax2.set_title(\"EOL NASA Score (mean, per engine)\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    for i, (fd, val) in enumerate(zip(eol_labels, eol_nasa_mean)):\n",
        "        if val != 0:\n",
        "            ax2.text(i, val, f\"{val:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Zusammenfassung\n",
        "# ===================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING COMPLETE - Summary\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTrained {len(FD_DATASETS)} separate models:\")\n",
        "for fd_id in FD_DATASETS:\n",
        "    m_point = results_dict[fd_id][\"metrics\"][\"pointwise\"]\n",
        "    print(f\"  {fd_id}: RMSE={m_point['rmse']:.4f}, MAE={m_point['mae']:.4f}, R²={m_point['r2']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"All results stored in 'results_dict' dictionary\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "turbine_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
